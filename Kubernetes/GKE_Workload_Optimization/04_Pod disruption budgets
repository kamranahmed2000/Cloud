---------------------------------------------------
#Pod disruption budgets
#Part of ensuring reliability and uptime for your GKE application relies on leveraging pod disruption budgets (pdp). PodDisruptionBudget is a Kubernetes resource that limits the number of pods of a replicated application that can be down simultaneously due to voluntary disruptions.

#Voluntary disruptions include administrative actions like deleting a deployment, updating a deployment's pod template and performing a rolling update, draining nodes that an application's pods reside on, or moving pods to different nodes.


---------------------------------------------------
#Delete your single pod app:
kubectl delete pod gb-frontend

#And generate a manifest that will create it as a deployment of 5 replicas:
cat << EOF > gb_frontend_deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gb-frontend
  labels:
    run: gb-frontend
spec:
  replicas: 5
  selector:
    matchLabels:
      run: gb-frontend
  template:
    metadata:
      labels:
        run: gb-frontend
    spec:
      containers:
        - name: gb-frontend
          image: gcr.io/google-samples/gb-frontend-amd64:v5
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
          ports:
            - containerPort: 80
              protocol: TCP
EOF
---------------------------------------------------
kubectl apply -f gb_frontend_deployment.yaml
---------------------------------------------------
#Drain the nodes by looping through the output of the default-pool's nodes and running the kubectl drain command on each individual node:
for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do
  kubectl drain --force --ignore-daemonsets --grace-period=10 "$node";
done

---------------------------------------------------
#Once your node has been drained, check in on your gb-frontend deployment's replica count:
kubectl describe deployment gb-frontend | grep ^Replicas

#First bring the drained nodes back by uncordoning them. The command below allows pods to be scheduled on the node again:
for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do
  kubectl uncordon "$node";
done
---------------------------------------------------
#Once again check in on the status of your deployment
kubectl describe deployment gb-frontend | grep ^Replicas

#Create a pod disruption budget that will declare the minimum number of available pods to be 4:
kubectl create poddisruptionbudget gb-pdb --selector run=gb-frontend --min-available 4

#Once again, drain one of your cluster's nodes and observe the output:
for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do
  kubectl drain --timeout=30s --ignore-daemonsets --grace-period=10 "$node";
done

#Check your deployments status once again:
kubectl describe deployment gb-frontend | grep ^Replicas

---------------------------------------------------
